//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30033411
// Cuda compilation tools, release 11.4, V11.4.48
// Based on NVVM 7.0.1
//

.version 7.4
.target sm_70
.address_size 64

	// .globl	_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi

.visible .entry _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi(
	.param .u64 _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_0,
	.param .u64 _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_1,
	.param .u64 _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_2,
	.param .u32 _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<49>;


	ld.param.u64 	%rd8, [_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_0];
	ld.param.u64 	%rd9, [_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_1];
	ld.param.u64 	%rd10, [_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_2];
	ld.param.u32 	%r5, [_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_3];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r7, %r6, %r8;
	setp.ge.s32 	%p1, %r1, %r5;
	@%p1 bra 	$L__BB0_3;

	cvta.to.global.u64 	%rd11, %rd10;
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd12, %rd9;
	mul.lo.s32 	%r2, %r1, 1858;
	mul.lo.s32 	%r10, %r1, 96;
	or.b32  	%r11, %r10, 8;
	mul.wide.s32 	%rd13, %r11, 2;
	add.s64 	%rd48, %rd12, %rd13;
	mul.wide.s32 	%rd14, %r11, 4;
	add.s64 	%rd47, %rd11, %rd14;
	mov.u32 	%r44, 0;

$L__BB0_2:
	ld.global.s16 	%r12, [%rd48+-16];
	add.s32 	%r13, %r2, %r12;
	mul.wide.s32 	%rd15, %r13, 2;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u16 	%rs1, [%rd16];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	st.global.f32 	[%rd47+-32], %f1;
	ld.global.s16 	%r14, [%rd48+-14];
	add.s32 	%r15, %r2, %r14;
	mul.wide.s32 	%rd17, %r15, 2;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.u16 	%rs2, [%rd18];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	st.global.f32 	[%rd47+-28], %f2;
	ld.global.s16 	%r16, [%rd48+-12];
	add.s32 	%r17, %r2, %r16;
	mul.wide.s32 	%rd19, %r17, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u16 	%rs3, [%rd20];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	st.global.f32 	[%rd47+-24], %f3;
	ld.global.s16 	%r18, [%rd48+-10];
	add.s32 	%r19, %r2, %r18;
	mul.wide.s32 	%rd21, %r19, 2;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u16 	%rs4, [%rd22];
	// begin inline asm
	{  cvt.f32.f16 %f4, %rs4;}

	// end inline asm
	st.global.f32 	[%rd47+-20], %f4;
	ld.global.s16 	%r20, [%rd48+-8];
	add.s32 	%r21, %r2, %r20;
	mul.wide.s32 	%rd23, %r21, 2;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u16 	%rs5, [%rd24];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs5;}

	// end inline asm
	st.global.f32 	[%rd47+-16], %f5;
	ld.global.s16 	%r22, [%rd48+-6];
	add.s32 	%r23, %r2, %r22;
	mul.wide.s32 	%rd25, %r23, 2;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.u16 	%rs6, [%rd26];
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs6;}

	// end inline asm
	st.global.f32 	[%rd47+-12], %f6;
	ld.global.s16 	%r24, [%rd48+-4];
	add.s32 	%r25, %r2, %r24;
	mul.wide.s32 	%rd27, %r25, 2;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.u16 	%rs7, [%rd28];
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs7;}

	// end inline asm
	st.global.f32 	[%rd47+-8], %f7;
	ld.global.s16 	%r26, [%rd48+-2];
	add.s32 	%r27, %r2, %r26;
	mul.wide.s32 	%rd29, %r27, 2;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.u16 	%rs8, [%rd30];
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs8;}

	// end inline asm
	st.global.f32 	[%rd47+-4], %f8;
	ld.global.s16 	%r28, [%rd48];
	add.s32 	%r29, %r2, %r28;
	mul.wide.s32 	%rd31, %r29, 2;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.u16 	%rs9, [%rd32];
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs9;}

	// end inline asm
	st.global.f32 	[%rd47], %f9;
	ld.global.s16 	%r30, [%rd48+2];
	add.s32 	%r31, %r2, %r30;
	mul.wide.s32 	%rd33, %r31, 2;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.u16 	%rs10, [%rd34];
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs10;}

	// end inline asm
	st.global.f32 	[%rd47+4], %f10;
	ld.global.s16 	%r32, [%rd48+4];
	add.s32 	%r33, %r2, %r32;
	mul.wide.s32 	%rd35, %r33, 2;
	add.s64 	%rd36, %rd1, %rd35;
	ld.global.u16 	%rs11, [%rd36];
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs11;}

	// end inline asm
	st.global.f32 	[%rd47+8], %f11;
	ld.global.s16 	%r34, [%rd48+6];
	add.s32 	%r35, %r2, %r34;
	mul.wide.s32 	%rd37, %r35, 2;
	add.s64 	%rd38, %rd1, %rd37;
	ld.global.u16 	%rs12, [%rd38];
	// begin inline asm
	{  cvt.f32.f16 %f12, %rs12;}

	// end inline asm
	st.global.f32 	[%rd47+12], %f12;
	ld.global.s16 	%r36, [%rd48+8];
	add.s32 	%r37, %r2, %r36;
	mul.wide.s32 	%rd39, %r37, 2;
	add.s64 	%rd40, %rd1, %rd39;
	ld.global.u16 	%rs13, [%rd40];
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs13;}

	// end inline asm
	st.global.f32 	[%rd47+16], %f13;
	ld.global.s16 	%r38, [%rd48+10];
	add.s32 	%r39, %r2, %r38;
	mul.wide.s32 	%rd41, %r39, 2;
	add.s64 	%rd42, %rd1, %rd41;
	ld.global.u16 	%rs14, [%rd42];
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs14;}

	// end inline asm
	st.global.f32 	[%rd47+20], %f14;
	ld.global.s16 	%r40, [%rd48+12];
	add.s32 	%r41, %r2, %r40;
	mul.wide.s32 	%rd43, %r41, 2;
	add.s64 	%rd44, %rd1, %rd43;
	ld.global.u16 	%rs15, [%rd44];
	// begin inline asm
	{  cvt.f32.f16 %f15, %rs15;}

	// end inline asm
	st.global.f32 	[%rd47+24], %f15;
	ld.global.s16 	%r42, [%rd48+14];
	add.s32 	%r43, %r2, %r42;
	mul.wide.s32 	%rd45, %r43, 2;
	add.s64 	%rd46, %rd1, %rd45;
	ld.global.u16 	%rs16, [%rd46];
	// begin inline asm
	{  cvt.f32.f16 %f16, %rs16;}

	// end inline asm
	st.global.f32 	[%rd47+28], %f16;
	add.s64 	%rd48, %rd48, 32;
	add.s64 	%rd47, %rd47, 64;
	add.s32 	%r44, %r44, 16;
	setp.ne.s32 	%p2, %r44, 96;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	ret;

}
	// .globl	_ZN5ceres18shiftConvertKernelEP6__halfPcffi
.visible .entry _ZN5ceres18shiftConvertKernelEP6__halfPcffi(
	.param .u64 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_0,
	.param .u64 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_1,
	.param .f32 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_2,
	.param .f32 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_3,
	.param .u32 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_0];
	ld.param.u64 	%rd2, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_1];
	ld.param.f32 	%f1, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_2];
	ld.param.f32 	%f2, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_3];
	ld.param.u32 	%r2, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	shl.b32 	%r6, %r1, 1;
	cvt.s64.s32 	%rd5, %r6;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.s8 	%rs2, [%rd6+1];
	mul.wide.s16 	%r7, %rs2, 256;
	ld.global.s8 	%r8, [%rd6];
	add.s32 	%r9, %r7, %r8;
	cvt.rn.f32.s32 	%f4, %r9;
	sub.f32 	%f5, %f2, %f1;
	mul.f32 	%f6, %f5, %f4;
	div.rn.f32 	%f7, %f6, 0f477FFF00;
	add.f32 	%f3, %f7, %f1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f3;}

	// end inline asm
	mul.wide.s32 	%rd7, %r1, 2;
	add.s64 	%rd8, %rd4, %rd7;
	st.global.u16 	[%rd8], %rs1;

$L__BB1_2:
	ret;

}

